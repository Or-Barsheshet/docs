---
title: "ðŸ”µExplore Model Catalog"
slug: "enrichment-models"
excerpt: ""
hidden: true
createdAt: "Tue Mar 11 2025 16:57:51 GMT+0000 (Coordinated Universal Time)"
updatedAt: "Wed Apr 09 2025 07:20:47 GMT+0000 (Coordinated Universal Time)"
---
With Visual Layerâ€™s data enrichment hub, you can enhance your data by generating metadata using advanced, cutting-edge models. This will enable you to:

- Gain deeper insights and unlock more value from your data.
- Easily search, filter, and segment your visual data based on the newly generated metadata.
- Effortlessly curate and organize your data.

# Model Catalog

Enhance your dataset with a diverse set of enrichment models, tailored to extract deeper insights:

|           Model Name          |          Task Type         |                                               Description                                              |
| :---------------------------: | :------------------------: | :----------------------------------------------------------------------------------------------------: |
|     **VL-Object-Detector**    |      Object Detection      | Identifies and locates objects within images or videos by drawing bounding boxes and classifying them. |
|      **VL-Image-Tagger**      | Multi-Class Classification |    Assigns labels or tags to an entire image, categorizing content for identification and analysis.    |
|    **VL-Object-Captioner**    |       Object to Text       |                        Generates descriptive text summarizing detected objects.                        |
|     **VL-Image-Captioner**    |        Image to Text       |           Generates descriptive text summarizing the content and context of an entire image.           |
|  **VL-Image-Semantic Search** |    Semantic Image Search   |      Allows image search with conceptual queries, identifying content that matches search intent.      |
| **VL-Object-Semantic Search** |   Semantic Object Search   |            Finds objects in images/videos based on meaning and context, beyond simple tags.            |
|       **NVILA-Lite-2B**       |  Image-Text-to-Text (VQA)  |           Optimized for efficiency and accuracy in video understanding and multi-image tasks.          |
|        **Janus-Pro-1B**       |  Image-Text-to-Text (VQA)  |               Autoregressive framework for multimodal understanding and text generation.               |

> **Note: Object/Image Captioner **and **Object/Image Semantic Search** require labels before enrichmentâ€”whether they come from user annotations, the Object Detector model, or the Image Tagger model.

***

## Coming Soon Models

The following models are currently in development and will be available soon in the enrichment catalog:

**GPT-4o** â€“ Multimodal LLM for advanced captioning and reasoning

**Nv-grounding dino** â€“ Grounded object detection with natural language queries

**Neva-22b** â€“ High-performance image-to-text transformer

**Nvclip** â€“ Vision-language model for fast and efficient embeddings

**Qwen-VL-2B** â€“ Vision-language model for captioning and answering questions

**Moondream2** â€“ Lightweight vision-language understanding

**Molmo-7B **â€“ Multimodal model for contextual enrichment

**Advanced-Image-Search** â€“ Conceptual and semantic image retrieval

**Advanced-Object-Search** â€“ Enhanced object-level semantic search

**VL-Object-Tagger** â€“ Model to assign multiple tags at the object level

<br />

> **Have questions or want early access? [Contact us](<>) for more information.**
